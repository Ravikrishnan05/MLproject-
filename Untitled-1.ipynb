{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hi i Am Ravikrishnan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Simulate data\n",
    "num_users = 100\n",
    "num_articles = 500\n",
    "num_categories = 10\n",
    "\n",
    "# Simulated user features\n",
    "user_features = np.random.rand(num_users, num_categories)  # Random category preferences\n",
    "user_data = pd.DataFrame(user_features, columns=[f'Category_{i}' for i in range(num_categories)])\n",
    "user_data['User ID'] = range(num_users)\n",
    "\n",
    "# Simulated article features\n",
    "article_features = np.random.randint(0, 2, size=(num_articles, num_categories))  # Random categories\n",
    "article_data = pd.DataFrame(article_features, columns=[f'Category_{i}' for i in range(num_categories)])\n",
    "article_data['Article ID'] = range(num_articles)\n",
    "\n",
    "# Simulated interactions (target)\n",
    "interactions = np.random.rand(num_users, num_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ravi krishnan A.M\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ravi krishnan A.M\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ravi krishnan A.M\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Ravi krishnan A.M\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Ravi krishnan A.M\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBartForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForSequenceClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User_data:\n",
      "   id                      name  \\\n",
      "0   1             Leanne Graham   \n",
      "1   2              Ervin Howell   \n",
      "2   3          Clementine Bauch   \n",
      "3   4          Patricia Lebsack   \n",
      "4   5          Chelsey Dietrich   \n",
      "5   6      Mrs. Dennis Schulist   \n",
      "6   7           Kurtis Weissnat   \n",
      "7   8  Nicholas Runolfsdottir V   \n",
      "8   9           Glenna Reichert   \n",
      "9  10        Clementina DuBuque   \n",
      "\n",
      "                                         news titles  Sports  Medical  Tech  \\\n",
      "0  [sunt aut facere repellat provident occaecati ...       1        4     4   \n",
      "1  [et ea vero quia laudantium autem, in quibusda...       4        5     2   \n",
      "2  [asperiores ea ipsam voluptatibus modi minima ...       4        3     4   \n",
      "3  [ullam ut quidem id aut vel consequuntur, dolo...       5        1     1   \n",
      "4  [non est facere, commodi ullam sint et exceptu...       1        3     4   \n",
      "5  [soluta aliquam aperiam consequatur illo quis ...       4        2     4   \n",
      "6  [voluptatem doloribus consectetur est ut ducim...       1        2     5   \n",
      "7  [et iusto veniam et illum aut fuga, sint hic d...       2        1     4   \n",
      "8  [tempora rem veritatis voluptas quo dolores ve...       5        1     4   \n",
      "9  [aut amet sed, ratione ex tenetur perferendis,...       1        5     5   \n",
      "\n",
      "   Watch Time  \n",
      "0       23.64  \n",
      "1       21.71  \n",
      "2       23.58  \n",
      "3       16.81  \n",
      "4        3.35  \n",
      "5       10.71  \n",
      "6        0.13  \n",
      "7       14.25  \n",
      "8        3.33  \n",
      "9        2.16  \n",
      "User_features:\n",
      "   id  Sports  Medical  Tech\n",
      "0   1       1        4     4\n",
      "1   2       4        5     2\n",
      "2   3       4        3     4\n",
      "3   4       5        1     1\n",
      "4   5       1        3     4\n",
      "5   6       4        2     4\n",
      "6   7       1        2     5\n",
      "7   8       2        1     4\n",
      "8   9       5        1     4\n",
      "9  10       1        5     5\n",
      "\n",
      "article_data:\n",
      "                                           News Title  Sports  Medical  Tech\n",
      "0   sunt aut facere repellat provident occaecati e...       0        0     1\n",
      "1                                        qui est esse       0        0     1\n",
      "2   ea molestias quasi exercitationem repellat qui...       1        0     0\n",
      "3                                eum et est occaecati       0        0     1\n",
      "4                                  nesciunt quas odio       0        0     1\n",
      "..                                                ...     ...      ...   ...\n",
      "95  quaerat velit veniam amet cupiditate aut numqu...       0        0     1\n",
      "96         quas fugiat ut perspiciatis vero provident       0        0     1\n",
      "97                        laboriosam dolor voluptates       0        1     0\n",
      "98  temporibus sit alias delectus eligendi possimu...       0        0     1\n",
      "99              at nam consequatur ea labore ea harum       0        1     0\n",
      "\n",
      "[100 rows x 4 columns]\n",
      "\n",
      "article_features:\n",
      "    Sports  Medical  Tech\n",
      "0        0        0     1\n",
      "1        0        0     1\n",
      "2        1        0     0\n",
      "3        0        0     1\n",
      "4        0        0     1\n",
      "..     ...      ...   ...\n",
      "95       0        0     1\n",
      "96       0        0     1\n",
      "97       0        1     0\n",
      "98       0        0     1\n",
      "99       0        1     0\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import requests #library to import api data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from transformers import pipeline #NLP\n",
    "\n",
    "# Fetch users and posts from JSONPlaceholder\n",
    "#user data \n",
    "users_url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "#news\n",
    "posts_url = \"https://jsonplaceholder.typicode.com/posts\"\n",
    "\n",
    "# Get users and posts data\n",
    "users_response = requests.get(users_url) #it will get the data \n",
    "posts_response = requests.get(posts_url)\n",
    "\n",
    "# Initialize the Hugging Face zero-shot classification pipeline\n",
    "model_name = \"facebook/bart-large-mnli\"    #the name of the nlp model\n",
    "classifier = pipeline(\"zero-shot-classification\", model=model_name, device=0) #training\n",
    "\n",
    "#device=0  uses gpu \n",
    "\n",
    "# Define candidate labels (your categories)\n",
    "candidate_labels = [\"Sports\", \"Medical\", \"Tech\"]\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "num_users = 10\n",
    "num_articles = 100\n",
    "num_categories = 4\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "if users_response.status_code == 200 and posts_response.status_code == 200:#checking data extracted,checks whether http requests are successful\n",
    "    users = users_response.json() #.json() converts it to list of dictionaries\n",
    "    posts = posts_response.json()\n",
    "    #we will get 2 list of dictionary for one is users and one is posts\n",
    "    # Prepare data: Create a list of dictionaries with 'id', 'name' and 'news titles'\n",
    "    data = []\n",
    "    for user in users:\n",
    "        user_id = user['id']\n",
    "        user_name = user['name']\n",
    "\n",
    "        # Filter posts by the user_id\n",
    "        user_posts = [post['title'] for post in posts if post['userId'] == user_id]\n",
    "\n",
    "        # Create a dictionary for each user with their name, list of post titles, and random ratings\n",
    "        data.append({\n",
    "            'id': user_id, \n",
    "            'name': user_name, \n",
    "            'news titles': user_posts,  # Store the list of titles here\n",
    "            'Sports': random.randint(1, 5),  # Random ratings between 1 and 5\n",
    "            'Medical': random.randint(1, 5), \n",
    "            'Tech': random.randint(1, 5),\n",
    "            'Watch Time': round(random.uniform(0, 24), 2)  # Random float value between 0 and 24, rounded to 2 decimals\n",
    "        })\n",
    "\n",
    "    # Create DataFrame for users with ratings\n",
    "    user_df = pd.DataFrame(data)\n",
    "\n",
    "    # Create a new DataFrame for news titles with Sports, Medical, and Tech\n",
    "    title_data = []\n",
    "\n",
    "    for user in users:\n",
    "        user_id = user['id']\n",
    "\n",
    "        # Filter posts by the user_id\n",
    "        user_posts = [post['title'] for post in posts if post['userId'] == user_id]\n",
    "\n",
    "        for title in user_posts:\n",
    "            # Use the Hugging Face classifier to classify each news title\n",
    "            result = classifier(title, candidate_labels)\n",
    "            category = result['labels'][0]  # Get the most likely category\n",
    "            score = result['scores'][0]  # Confidence score\n",
    "\n",
    "            # Based on the classification, assign binary values for each category\n",
    "            sports = 1 if category == \"Sports\" else 0\n",
    "            medical = 1 if category == \"Medical\" else 0\n",
    "            tech = 1 if category == \"Tech\" else 0\n",
    "\n",
    "            # Add the title and category values to the list\n",
    "            title_data.append({\n",
    "                'News Title': title,\n",
    "                'Sports': sports,\n",
    "                'Medical': medical,\n",
    "                'Tech': tech,\n",
    "            })\n",
    "\n",
    "    # Create DataFrame for the news titles with binary values for each category\n",
    "    title_df = pd.DataFrame(title_data)\n",
    "    user_features=user_df.drop(columns=['name','news titles','Watch Time'])\n",
    "    user_data=user_df\n",
    "    article_features=title_df.drop(columns=['News Title'])\n",
    "    article_data=title_df\n",
    "    # Display the DataFrames\n",
    "    print(\"User_data:\")\n",
    "    print(user_data)\n",
    "    print(\"User_features:\")\n",
    "    print(user_features)\n",
    "    print(\"\\narticle_data:\")\n",
    "    print(article_data)\n",
    "    print(\"\\narticle_features:\")\n",
    "    print(article_features)\n",
    "    interactions = np.random.rand(num_users, num_articles)    \n",
    "else:\n",
    "    print(\"Error fetching data from APIs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category_0</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Category_3</th>\n",
       "      <th>Category_4</th>\n",
       "      <th>Category_5</th>\n",
       "      <th>Category_6</th>\n",
       "      <th>Category_7</th>\n",
       "      <th>Category_8</th>\n",
       "      <th>Category_9</th>\n",
       "      <th>User ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.880129</td>\n",
       "      <td>0.745096</td>\n",
       "      <td>0.906387</td>\n",
       "      <td>0.867032</td>\n",
       "      <td>0.280362</td>\n",
       "      <td>0.923997</td>\n",
       "      <td>0.594500</td>\n",
       "      <td>0.032999</td>\n",
       "      <td>0.234221</td>\n",
       "      <td>0.353996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.733979</td>\n",
       "      <td>0.817367</td>\n",
       "      <td>0.268438</td>\n",
       "      <td>0.544903</td>\n",
       "      <td>0.891306</td>\n",
       "      <td>0.774258</td>\n",
       "      <td>0.951155</td>\n",
       "      <td>0.383836</td>\n",
       "      <td>0.384857</td>\n",
       "      <td>0.943172</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042992</td>\n",
       "      <td>0.189237</td>\n",
       "      <td>0.835230</td>\n",
       "      <td>0.971827</td>\n",
       "      <td>0.902835</td>\n",
       "      <td>0.555725</td>\n",
       "      <td>0.158756</td>\n",
       "      <td>0.888381</td>\n",
       "      <td>0.373437</td>\n",
       "      <td>0.822511</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.393547</td>\n",
       "      <td>0.270209</td>\n",
       "      <td>0.642507</td>\n",
       "      <td>0.085269</td>\n",
       "      <td>0.995769</td>\n",
       "      <td>0.576927</td>\n",
       "      <td>0.243533</td>\n",
       "      <td>0.507052</td>\n",
       "      <td>0.142420</td>\n",
       "      <td>0.992215</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.319711</td>\n",
       "      <td>0.043622</td>\n",
       "      <td>0.437376</td>\n",
       "      <td>0.729023</td>\n",
       "      <td>0.469597</td>\n",
       "      <td>0.829861</td>\n",
       "      <td>0.242999</td>\n",
       "      <td>0.714857</td>\n",
       "      <td>0.327980</td>\n",
       "      <td>0.922044</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category_0  Category_1  Category_2  Category_3  Category_4  Category_5  \\\n",
       "0    0.880129    0.745096    0.906387    0.867032    0.280362    0.923997   \n",
       "1    0.733979    0.817367    0.268438    0.544903    0.891306    0.774258   \n",
       "2    0.042992    0.189237    0.835230    0.971827    0.902835    0.555725   \n",
       "3    0.393547    0.270209    0.642507    0.085269    0.995769    0.576927   \n",
       "4    0.319711    0.043622    0.437376    0.729023    0.469597    0.829861   \n",
       "\n",
       "   Category_6  Category_7  Category_8  Category_9  User ID  \n",
       "0    0.594500    0.032999    0.234221    0.353996        0  \n",
       "1    0.951155    0.383836    0.384857    0.943172        1  \n",
       "2    0.158756    0.888381    0.373437    0.822511        2  \n",
       "3    0.243533    0.507052    0.142420    0.992215        3  \n",
       "4    0.242999    0.714857    0.327980    0.922044        4  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_data.head(5)\n",
    "#user_data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category_0</th>\n",
       "      <th>Category_1</th>\n",
       "      <th>Category_2</th>\n",
       "      <th>Category_3</th>\n",
       "      <th>Category_4</th>\n",
       "      <th>Category_5</th>\n",
       "      <th>Category_6</th>\n",
       "      <th>Category_7</th>\n",
       "      <th>Category_8</th>\n",
       "      <th>Category_9</th>\n",
       "      <th>Article ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category_0  Category_1  Category_2  Category_3  Category_4  Category_5  \\\n",
       "0           1           0           1           1           0           1   \n",
       "1           0           0           0           0           0           1   \n",
       "2           1           1           1           0           0           1   \n",
       "3           0           1           0           1           0           0   \n",
       "4           1           1           1           1           1           1   \n",
       "\n",
       "   Category_6  Category_7  Category_8  Category_9  Article ID  \n",
       "0           1           1           0           0           0  \n",
       "1           1           0           1           0           1  \n",
       "2           1           1           0           1           2  \n",
       "3           0           1           0           0           3  \n",
       "4           1           0           1           1           4  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_data.head(5)\n",
    "#article_data.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.37717426, 0.12242754, 0.07019741, ..., 0.71399086, 0.18146716,\n",
       "        0.49133705],\n",
       "       [0.36352044, 0.95922666, 0.00855586, ..., 0.07414869, 0.30836361,\n",
       "        0.49658657],\n",
       "       [0.8519202 , 0.37779632, 0.16126675, ..., 0.01677642, 0.59620217,\n",
       "        0.41249535],\n",
       "       ...,\n",
       "       [0.47212028, 0.70485327, 0.73297323, ..., 0.15308649, 0.92335142,\n",
       "        0.66279078],\n",
       "       [0.84779037, 0.55339759, 0.89679156, ..., 0.6335067 , 0.84897034,\n",
       "        0.84445014],\n",
       "       [0.78461169, 0.91872511, 0.90585676, ..., 0.49350335, 0.05135916,\n",
       "        0.16017432]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling user and article features\n",
    "scaler_user = StandardScaler()\n",
    "scaler_article = StandardScaler()\n",
    "\n",
    "user_features_scaled = scaler_user.fit_transform(user_features)\n",
    "article_features_scaled = scaler_article.fit_transform(article_features)\n",
    "\n",
    "# Scale interaction scores\n",
    "scaler_target = MinMaxScaler((0, 5))\n",
    "interaction_scaled = scaler_target.fit_transform(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_features_scaled.shape (100, 10)\n",
      "article_features_scaled.shape (500, 10)\n",
      "interaction_scaled.shape (100, 500)\n"
     ]
    }
   ],
   "source": [
    "print(\"user_features_scaled.shape\",user_features_scaled.shape)\n",
    "print(\"article_features_scaled.shape\",article_features_scaled.shape)\n",
    "print(\"interaction_scaled.shape\",interaction_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test sets\n",
    "user_train, user_test= train_test_split(\n",
    "    user_features_scaled, \n",
    "    train_size=0.8, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "article_train, article_test= train_test_split(\n",
    "article_features_scaled, \n",
    "    train_size=0.8, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "y_train, y_test = train_test_split(\n",
    "    interaction_scaled.flatten(),  # Flatten for single-dimension target\n",
    "    train_size=0.8, \n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_outputs = 32\n",
    "tf.random.set_seed(1)\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ###     \n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs)\n",
    "    ### END CODE HERE ###  \n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    ### START CODE HERE ###     \n",
    "    tf.keras.layers.Dense(256,activation='relu'),\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs)\n",
    "    ### END CODE HERE ###  \n",
    "])\n",
    "\n",
    "# create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.linalg.l2_normalize(vu, axis=1)\n",
    "\n",
    "# create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.linalg.l2_normalize(vm, axis=1)\n",
    "\n",
    "# compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])\n",
    "\n",
    "# specify the inputs and output of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "cost_fn = tf.keras.losses.MeanSquaredError()\n",
    "opt = keras.optimizers.Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=opt,\n",
    "              loss=cost_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1)\n",
    "model.fit([user_train[:, u_s:], item_train[:, i_s:]], y_train, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([user_test[:, u_s:], item_test[:, i_s:]], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here write the prediction code  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
